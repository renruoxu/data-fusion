{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't import dot_parser, loading of dot files will not be possible.\n"
     ]
    }
   ],
   "source": [
    "import theano.tensor as T\n",
    "import theano\n",
    "import numpy as np\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from theano.printing import pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class HiddenLayer(object):\n",
    "    def __init__(self, rng, input, n_in, n_out, W = None, b = None, activation = T.tanh):\n",
    "        \"\"\"\n",
    "        HIdden layer of MAP\n",
    "        \n",
    "        rng: for random state --> np.random.RandomState\n",
    "        \"\"\"\n",
    "        self.input = input\n",
    "        if W is None:\n",
    "            W_values = np.asarray(\n",
    "                rng.uniform(\n",
    "                    low = - np.sqrt(6. / (n_in + n_out)),\n",
    "                    high = np.sqrt(6. / (n_in + n_out)),\n",
    "                    size = (n_in, n_out)\n",
    "                    ),\n",
    "                dtype = theano.config.floatX\n",
    "                )\n",
    "            if activation == theano.tensor.nnet.sigmoid:\n",
    "                W_values *= 4.\n",
    "            \n",
    "            W1 = theano.shared(W_values, name='W1', borrow = True)\n",
    "        \n",
    "        if b is None:\n",
    "            b_values = np.zeros((n_out,), dtype=theano.config.floatX)\n",
    "            b1 = theano.shared(value=b_values, name='b1', borrow=True)\n",
    "            \n",
    "        self.W = W1\n",
    "        self.b = b1\n",
    "            \n",
    "        lin_output = T.dot(input,self.W) + self.b\n",
    "        self.output = (\n",
    "            lin_output if activation is None\n",
    "            else activation(lin_output)\n",
    "            )\n",
    "            \n",
    "        self.params = [self.W, self.b]\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class OutputLayer(object):\n",
    "    def __init__(self, rng, input, n_in, n_out, W = None, b = None, activation = None):\n",
    "        \"\"\"\n",
    "        Output layer of MAP\n",
    "        \n",
    "        rng: for random state --> np.random.RandomState\n",
    "        \"\"\"\n",
    "        self.input = input\n",
    "        \n",
    "        if W is None:\n",
    "            W_values = np.asarray(\n",
    "                rng.uniform(\n",
    "                    low = - np.sqrt(6. / (n_in + n_out)),\n",
    "                    high = np.sqrt(6. / (n_in + n_out)),\n",
    "                    size = (n_in, n_out)\n",
    "                    ),\n",
    "                dtype = theano.config.floatX\n",
    "                )\n",
    "            if activation == theano.tensor.nnet.sigmoid:\n",
    "                W_values *= 4\n",
    "            \n",
    "            W2 = theano.shared(W_values, name='W2', borrow = True)\n",
    "            \n",
    "        if b is None:\n",
    "            b_values = np.zeros((n_out,), dtype=theano.config.floatX)\n",
    "            b2 = theano.shared(value=b_values, name='b2', borrow=True)\n",
    "            \n",
    "        self.W = W2\n",
    "        self.b = b2\n",
    "            \n",
    "        lin_output = T.dot(input,self.W) + self.b\n",
    "        self.output = (\n",
    "            lin_output if activation is None\n",
    "            else activation(lin_output)\n",
    "            )\n",
    "            \n",
    "        self.params = [self.W, self.b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MAP(object):\n",
    "    \n",
    "    def __init__(self, rng, input, n_in, n_hidden, n_out):\n",
    "        \"\"\" Initialization\n",
    "        \"\"\"\n",
    "        self.hiddenLayer = HiddenLayer(\n",
    "            rng = rng,\n",
    "            input = input,\n",
    "            n_in = n_in,\n",
    "            n_out = n_hidden,\n",
    "            activation = T.tanh\n",
    "            )\n",
    "        \n",
    "        self.outputLayer = OutputLayer(\n",
    "            rng = rng,\n",
    "            input = self.hiddenLayer.output,\n",
    "            n_in = n_hidden,\n",
    "            n_out = n_out\n",
    "            )\n",
    "        \n",
    "        self.params = self.hiddenLayer.params + self.outputLayer.params\n",
    "        \n",
    "    def L2_regulation(self,y):\n",
    "        \"\"\" Returns the L2 regulation of object vector and predicted vetor\n",
    "        \"\"\"\n",
    "        return T.sum((y - self.outputLayer.output) ** 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class MAPPING(object):\n",
    "    \"\"\" Returns the mapping function \n",
    "    X: np.array --> n_samples x n_features\n",
    "    Y: np.array --> n_samples x n_features\n",
    "    lam: penalization for weights\n",
    "    beta: weight for sparsity penalization\n",
    "    sparsity_param: desired average activation of the hidden units\n",
    "    \"\"\"\n",
    "    def __init__(self,learning_rate = 0.01, n_epochs = 1000, batch_size = 20, \n",
    "                 n_hidden = 200, lam = 1e-4, beta = 0.5, sparsity_param = 0.05):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_epochs = n_epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.n_hidden = n_hidden\n",
    "        self.lam = lam\n",
    "        self.beta = beta\n",
    "        self.sparsity_param = sparsity_param\n",
    "        self.W1  = None\n",
    "        self.W2 = None\n",
    "        self.b1 = None\n",
    "        self.b2 = None\n",
    "        self.x_mean = None\n",
    "        self.x_std = None\n",
    "        self.y_mean = None\n",
    "        self.y_std = None\n",
    "        self.gradients = None\n",
    "        self.L2 = None\n",
    "        self.sparsity = None\n",
    "        self.reg = None\n",
    "        \n",
    "    \n",
    "    def fit(self,train_X,train_Y, validate_X = None, validate_Y = None, test_X = None, test_Y = None, debug = False):\n",
    "        \"\"\"\n",
    "        debug: If true, in debug mode\n",
    "        \"\"\"\n",
    "        \n",
    "        #self.x_mean = train_X.mean()\n",
    "        #self.x_std = train_X.std()\n",
    "        #self.y_mean = train_Y.mean()\n",
    "        #self.y_std = train_Y.std()\n",
    "        #train_X = (train_X - self.x_mean) / self.x_std\n",
    "        #train_Y = (train_Y - self.y_mean) / self.y_std\n",
    "        shared_train_X = theano.shared(np.asarray(train_X, dtype = theano.config.floatX))\n",
    "        shared_train_Y = theano.shared(np.asarray(train_Y, dtype = theano.config.floatX))\n",
    "        n_train_batches = train_X.shape[0] / self.batch_size\n",
    "        \n",
    "        if valid_X != None and valid_Y != None:\n",
    "            n_valid_batches = validate_X.shape[0] / self.batch_size\n",
    "            #validate_X = (validate_X - self.x_mean) / self.x_std\n",
    "            #validate_Y = (validate_Y - self.y_mean) / self.y_std\n",
    "            shared_validate_X = theano.shared(np.asarray(validate_X, dtype = theano.config.floatX))\n",
    "            shared_validate_Y = theano.shared(np.asarray(validate_Y, dtype = theano.config.floatX))\n",
    "            \n",
    "        \n",
    "        ### Build model ###\n",
    "        print \"... building the model...\"\n",
    "\n",
    "        # allocate symbolic variables for the data\n",
    "        index = T.lscalar() # index for a mini batch\n",
    "        x = T.dmatrix('x')\n",
    "        y = T.dmatrix('y')\n",
    "\n",
    "        rng = np.random.RandomState(1234)\n",
    "\n",
    "        # construct MAP class\n",
    "        f_mapping = MAP(\n",
    "            rng = rng,\n",
    "            input = x,\n",
    "            n_in = train_X.shape[1],\n",
    "            n_hidden = self.n_hidden,\n",
    "            n_out = train_Y.shape[1]\n",
    "            )\n",
    "        \n",
    "        self.W1 = f_mapping.hiddenLayer.W.get_value()\n",
    "        self.b1 = f_mapping.hiddenLayer.b.get_value()\n",
    "        self.W2 = f_mapping.outputLayer.W.get_value()\n",
    "        self.b2 = f_mapping.outputLayer.b.get_value()\n",
    "        \n",
    "\n",
    "        ### Cost function ########################################################\n",
    "        hidden_output = f_mapping.hiddenLayer.output\n",
    "        rho_hat = (T.sum(hidden_output, axis = 0) / train_X.shape[0] + 1) / 2 ## rescale to [0,1]\n",
    "        L2 = f_mapping.L2_regulation(y)/ self.batch_size * 0.5 # penalization for vector\n",
    "        reg = 0.5 * self.lam * (T.sum(f_mapping.hiddenLayer.W ** 2) + T.sum(f_mapping.hiddenLayer.b **2) + \n",
    "                                T.sum(f_mapping.outputLayer.W ** 2) + T.sum(f_mapping.outputLayer.b ** 2))\n",
    "        sparsity = self.beta * T.sum(self.sparsity_param * T.log(self.sparsity_param / rho_hat + 1e-6) + \n",
    "                                     (1. - self.sparsity_param) * T.log((1. - self.sparsity_param + 1e-6) / ( 1. - rho_hat)))\n",
    "        cost = L2  + reg + sparsity\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "        # compute the gradient of the cost wrt to parameters\n",
    "        gparams = [T.grad(cost,param) for param in f_mapping.params]\n",
    "        \n",
    "\n",
    "        # specify how to update parameters\n",
    "        updates = [\n",
    "            (param, param - self.learning_rate * gparam)\n",
    "            for param, gparam in zip(f_mapping.params, gparams)\n",
    "            ]\n",
    "        \n",
    "        train_model = theano.function(\n",
    "                inputs = [index],\n",
    "                outputs = [cost,L2, reg, sparsity, rho_hat],\n",
    "                updates = updates,\n",
    "                givens={\n",
    "                    x: shared_train_X[index * self.batch_size: (index + 1) * self.batch_size],\n",
    "                    y: shared_train_Y[index * self.batch_size: (index + 1) * self.batch_size]\n",
    "                    }\n",
    "                )\n",
    "        \n",
    "\n",
    "        \n",
    "        if validate_X != None and validate_Y != None:\n",
    "            validate_model = theano.function(\n",
    "                inputs = [],\n",
    "                outputs = f_mapping.L2_regulation(y),\n",
    "                givens = {\n",
    "                    x: shared_validate_X,\n",
    "                    y: shared_validate_Y,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "        \n",
    "        \n",
    "        ## function to train the parameters\n",
    "        if debug:\n",
    "            ## store intermediate values\n",
    "            \n",
    "            \n",
    "            initialization = theano.function(\n",
    "                inputs = [index],\n",
    "                outputs = [cost,L2, reg, sparsity],\n",
    "                givens={\n",
    "                    x: shared_train_X[index * self.batch_size: (index + 1) * self.batch_size],\n",
    "                    y: shared_train_Y[index * self.batch_size: (index + 1) * self.batch_size]\n",
    "                    }\n",
    "                )\n",
    "            \n",
    "            train_gparams = theano.function(\n",
    "                inputs = [index],\n",
    "                outputs = gparams,\n",
    "                givens={\n",
    "                    x: shared_train_X[index * self.batch_size: (index + 1) * self.batch_size],\n",
    "                    y: shared_train_Y[index * self.batch_size: (index + 1) * self.batch_size]\n",
    "                    }\n",
    "                )\n",
    "            \n",
    "        \n",
    "            \n",
    "           \n",
    "\n",
    "        ################### \n",
    "        #train the model\n",
    "        ###################\n",
    "        print \"..training the model...\"\n",
    "        \n",
    "        if validate_X != None and validate_Y != None:\n",
    "            patience = 7000\n",
    "            patience_increase = 2\n",
    "            improvement_threshold = 0.9995\n",
    "            validation_frequency = min(n_train_batches, patience / 2)\n",
    "            best_validation_loss = np.inf\n",
    "            done_looping = False\n",
    "        else:\n",
    "            min_cost = np.inf\n",
    "        best_W1 = f_mapping.hiddenLayer.W.get_value()\n",
    "        best_b1 = f_mapping.hiddenLayer.b.get_value()\n",
    "        best_W2 = f_mapping.outputLayer.W.get_value()\n",
    "        bset_b2 = f_mapping.outputLayer.b.get_value()\n",
    "        \n",
    "        \n",
    "        start_time = time.clock()\n",
    "        epoch = 0\n",
    "        \n",
    "        if debug:\n",
    "            initial_values = np.zeros((n_train_batches,4))\n",
    "            for i in xrange(n_train_batches):\n",
    "                initial_values[i] = initialization(i)\n",
    "            self.gradients = [train_gparams(i) for i in xrange(n_train_batches)]\n",
    "            self.cost = initial_values[:,0]\n",
    "            self.L2 = initial_values[:,1]\n",
    "            self.reg = initial_values[:,2]\n",
    "            self.sparsity =  initial_values[:,3]\n",
    "\n",
    "\n",
    "        while (epoch < self.n_epochs) and (not done_looping):\n",
    "            ## shuffle train_X, train_Y\n",
    "            train_X, train_Y = shuffle(train_X, train_Y)\n",
    "            shared_train_X = theano.shared(np.asarray(train_X, dtype = theano.config.floatX))\n",
    "            shared_train_Y = theano.shared(np.asarray(train_Y, dtype = theano.config.floatX))\n",
    "            \n",
    "            train_X, train_Y = shuffle(train_X, train_Y)\n",
    "            epoch += 1\n",
    "            minibatch_avg_cost = []\n",
    "            minibatch_avg_L2 = []\n",
    "            minibatch_avg_reg = []\n",
    "            minibatch_avg_sparisity = []\n",
    "            for minibatch_index in xrange(n_train_batches):\n",
    "                tmp_out = train_model(minibatch_index)\n",
    "                minibatch_avg_cost.append(tmp_out[0])\n",
    "                minibatch_avg_L2.append(tmp_out[1])\n",
    "                minibatch_avg_reg.append(tmp_out[2])\n",
    "                minibatch_avg_sparisity.append(tmp_out[3])\n",
    "                if debug:\n",
    "                    self.gradients = train_gparams(minibatch_index)\n",
    "                    \n",
    "                iter = (epoch - 1) * n_train_batches + minibatch_index\n",
    "\n",
    "                \n",
    "                if validate_X != None and validate_Y != None:\n",
    "                    if (iter + 1) % validation_frequency == 0:\n",
    "                        this_validation_loss = np.sqrt(validate_model() / validate_X.shape[0])\n",
    "\n",
    "\n",
    "                        print(\n",
    "                            'epoch %i, minibatch %i/%i, validation error %f'%\n",
    "                            (\n",
    "                                epoch,\n",
    "                                minibatch_index + 1,\n",
    "                                n_train_batches,\n",
    "                                this_validation_loss\n",
    "                                )\n",
    "                            )\n",
    "                        \n",
    "                        if debug:\n",
    "                            print(\n",
    "                                'epoch %i,minibatch %i/%i, cost, %f, L2 %f, reg %f, sparsity %f'%\n",
    "                                (epoch,\n",
    "                                 minibatch_index + 1,\n",
    "                                 n_train_batches,\n",
    "                                 np.average(minibatch_avg_cost),\n",
    "                                 np.average(minibatch_avg_L2),\n",
    "                                 np.average(minibatch_avg_reg),\n",
    "                                 np.average(minibatch_avg_sparisity)\n",
    "                                )\n",
    "                                )\n",
    "                            self.cost = minibatch_avg_cost\n",
    "                            self.L2 = minibatch_avg_L2\n",
    "                            self.reg = minibatch_avg_reg\n",
    "\n",
    "\n",
    "                    \n",
    "                    \n",
    "                        if this_validation_loss < best_validation_loss:\n",
    "                            self.best_W1 = f_mapping.hiddenLayer.W.get_value()\n",
    "                            self.best_b1 = f_mapping.hiddenLayer.b.get_value()\n",
    "                            self.best_W2 = f_mapping.outputLayer.W.get_value()\n",
    "                            self.best_b2 = f_mapping.outputLayer.b.get_value()\n",
    "                            self.best_validation_loss = this_validation_loss\n",
    "                            \n",
    "                            if this_validation_loss < best_validation_loss * improvement_threshold:\n",
    "                                patience = max(patience, iter * patience_increase)\n",
    "                            \n",
    "                            best_validation_loss = this_validation_loss\n",
    "\n",
    "                        \n",
    "                        if patience <= iter:\n",
    "                            done_looping = True\n",
    "                            break\n",
    "\n",
    "            \n",
    " \n",
    "                \n",
    "            \n",
    "        self.W1 = self.best_W1\n",
    "        self.W2 = self.best_W2\n",
    "        self.b1 = self.best_b1\n",
    "        self.b2 = self.best_b2\n",
    "                            \n",
    "        \n",
    "    \n",
    "    def transfrom(self,X):\n",
    "        \"\"\"\n",
    "        X: n_sample x n_features\n",
    "        \"\"\"\n",
    "        #X = (X - self.x_mean)/self.x_std\n",
    "        Y = np.dot(np.tanh(np.dot(X,self.W1) + self.b1 ), self.W2 ) + self.b2\n",
    "        return Y\n",
    "        #return Y * self.y_std + self.y_mean"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "with open(\"../data/cifar-10-batches-py/X_fc7_cudanet_out\",\"r\") as f:\n",
    "    X = cPickle.load(f)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "with open(\"../data/cifar-10-batches-py/Y_100\",\"r\") as f:\n",
    "    Y = cPickle.load(f)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "store = pd.HDFStore(\"../data/cifa_XY.hd5\")\n",
    "store['X'] = pd.DataFrame(X)\n",
    "store['Y'] = pd.DataFrame(Y)\n",
    "\n",
    "store.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "store = pd.HDFStore(\"../data/cifa_XY.hd5\")\n",
    "X = store['X'].values\n",
    "Y = store['Y'].values\n",
    "store.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(57000, 4096)\n",
      "(57000, 100)\n",
      "(3000, 4096)\n",
      "(3000, 100)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "tmp_X,test_X, tmp_Y, test_Y = train_test_split(X,np.array(Y),test_size = 0.05)\n",
    "print tmp_X.shape\n",
    "print tmp_Y.shape\n",
    "print test_X.shape\n",
    "print test_Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "train_X,valid_X, train_Y, valid_Y = train_test_split(tmp_X, tmp_Y,\n",
    "                                                     test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45600, 4096)\n",
      "(45600, 100)\n",
      "(11400, 4096)\n",
      "(11400, 100)\n"
     ]
    }
   ],
   "source": [
    "print train_X.shape\n",
    "print train_Y.shape\n",
    "print valid_X.shape\n",
    "print valid_Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45600, 4096)\n",
      "(45600, 100)\n",
      "(11400, 4096)\n",
      "(11400, 100)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "ss_X = StandardScaler()\n",
    "ss_Y = StandardScaler()\n",
    "ss_X.fit(train_X)\n",
    "ss_Y.fit(train_Y)\n",
    "train_X, valid_X = ss_X.transform(train_X), ss_X.transform(valid_X)\n",
    "train_Y, valid_Y = ss_Y.transform(train_Y), ss_Y.transform(valid_Y)\n",
    "print train_X.shape\n",
    "print train_Y.shape\n",
    "print valid_X.shape\n",
    "print valid_Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... building the model...\n",
      "..training the model...\n",
      "epoch 1, minibatch 912/912, validation error 6.436697\n",
      "epoch 1,minibatch 912/912, cost, 519.584004, L2 24.929279, reg 0.024084, sparsity 494.630641\n",
      "epoch 2, minibatch 912/912, validation error 6.102990\n",
      "epoch 2,minibatch 912/912, cost, 513.587556, L2 18.938922, reg 0.023533, sparsity 494.625101\n",
      "epoch 3, minibatch 912/912, validation error 5.936176\n",
      "epoch 3,minibatch 912/912, cost, 511.664594, L2 17.019233, reg 0.024084, sparsity 494.621277\n",
      "epoch 4, minibatch 912/912, validation error 5.836351\n",
      "epoch 4,minibatch 912/912, cost, 510.449611, L2 15.806045, reg 0.025088, sparsity 494.618479\n",
      "epoch 5, minibatch 912/912, validation error 5.779192\n",
      "epoch 5,minibatch 912/912, cost, 509.571295, L2 14.928527, reg 0.026359, sparsity 494.616409\n",
      "epoch 6, minibatch 912/912, validation error 5.738256\n",
      "epoch 6,minibatch 912/912, cost, 508.886342, L2 14.243487, reg 0.027799, sparsity 494.615056\n",
      "epoch 7, minibatch 912/912, validation error 5.724776\n",
      "epoch 7,minibatch 912/912, cost, 508.338574, L2 13.695264, reg 0.029342, sparsity 494.613969\n",
      "epoch 8, minibatch 912/912, validation error 5.710404\n",
      "epoch 8,minibatch 912/912, cost, 507.867800, L2 13.223450, reg 0.030950, sparsity 494.613401\n",
      "epoch 9, minibatch 912/912, validation error 5.706348\n",
      "epoch 9,minibatch 912/912, cost, 507.465245, L2 12.819766, reg 0.032596, sparsity 494.612883\n",
      "epoch 10, minibatch 912/912, validation error 5.706008\n",
      "epoch 10,minibatch 912/912, cost, 507.106089, L2 12.459419, reg 0.034267, sparsity 494.612403\n",
      "epoch 11, minibatch 912/912, validation error 5.704894\n",
      "epoch 11,minibatch 912/912, cost, 506.778060, L2 12.130142, reg 0.035958, sparsity 494.611959\n",
      "epoch 12, minibatch 912/912, validation error 5.711598\n",
      "epoch 12,minibatch 912/912, cost, 506.481094, L2 11.831914, reg 0.037661, sparsity 494.611520\n",
      "epoch 13, minibatch 912/912, validation error 5.722156\n",
      "epoch 13,minibatch 912/912, cost, 506.201248, L2 11.550714, reg 0.039371, sparsity 494.611163\n",
      "epoch 14, minibatch 912/912, validation error 5.727882\n",
      "epoch 14,minibatch 912/912, cost, 505.937171, L2 11.285216, reg 0.041091, sparsity 494.610864\n",
      "epoch 15, minibatch 912/912, validation error 5.733290\n",
      "epoch 15,minibatch 912/912, cost, 505.692288, L2 11.038867, reg 0.042814, sparsity 494.610607\n",
      "epoch 16, minibatch 912/912, validation error 5.744006\n",
      "epoch 16,minibatch 912/912, cost, 505.460507, L2 10.805683, reg 0.044536, sparsity 494.610288\n",
      "epoch 17, minibatch 912/912, validation error 5.750929\n",
      "epoch 17,minibatch 912/912, cost, 505.239116, L2 10.582849, reg 0.046256, sparsity 494.610011\n",
      "epoch 18, minibatch 912/912, validation error 5.761321\n",
      "epoch 18,minibatch 912/912, cost, 505.032446, L2 10.374689, reg 0.047971, sparsity 494.609786\n",
      "CPU times: user 1h 40min 28s, sys: 1h 45min 34s, total: 3h 26min 2s\n",
      "Wall time: 8min 31s\n"
     ]
    }
   ],
   "source": [
    "mapping = MAPPING(n_hidden = 200, n_epochs = 500, batch_size = 50, learning_rate=0.005, lam=1e-4, sparsity_param=0.05,\n",
    "                 beta = 5)\n",
    "%time mapping.fit(train_X,train_Y, validate_X = valid_X, validate_Y = valid_Y, test_X = test_X, test_Y= test_Y, debug = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.7048940267523296"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapping.best_validation_loss"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "x_y_params = {}\n",
    "x_y_params['x_mean'] = mapping.x_mean\n",
    "x_y_params['x_std'] = mapping.x_std\n",
    "x_y_params['y_mean'] = mapping.y_mean\n",
    "x_y_params['y_std'] = mapping.y_std"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "print x_y_params"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "with open(\"query/nn_model/W1\",\"wb\") as f:\n",
    "    cPickle.dump(mapping.W1,f)\n",
    "with open(\"query/nn_model/W2\",\"wb\") as f:\n",
    "    cPickle.dump(mapping.W2,f)\n",
    "with open(\"query/nn_model/b1\",\"wb\") as f:\n",
    "    cPickle.dump(mapping.b1,f)\n",
    "with open(\"query/nn_model/b2\",\"wb\") as f:\n",
    "    cPickle.dump(mapping.b2,f)\n",
    "with open(\"query/nn_model/params\",\"wb\") as f:\n",
    "    cPickle.dump(x_y_params,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import cPickle\n",
    "\n",
    "W1 = theano.shared(mapping.best_W1, name='W1', borrow = True)\n",
    "W2 = theano.shared(mapping.best_W2, name='W2', borrow = True)\n",
    "b1 = theano.shared(mapping.best_b1, name='b1', borrow = True)\n",
    "b2 = theano.shared(mapping.best_b2, name='b2', borrow = True)\n",
    "X = T.dvector(name='X')\n",
    "Y = (T.dot(T.tanh(T.dot((X - ss_X.mean_)*1./ss_X.std_,W1) + b1),W2) + b2) * ss_Y.std_ + ss_Y.mean_\n",
    "nn = theano.function([X],outputs = Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cPickle.dump(nn, open(\"../query/img2txt.pkl\", \"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "img2txt_model = cPickle.load(open(\"../query/img2txt.pkl\", \"r\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.12500292765668447"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(img2txt_model(test_X[0,:]) - test_Y[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.05 ms, sys: 42 Âµs, total: 1.09 ms\n",
      "Wall time: 1.1 ms\n"
     ]
    }
   ],
   "source": [
    "with open(\"/home/ce-ruoxu/workspace/data-fusion/data/cifar-10-batches-py/labels_vectors\",\"r\") as f:\n",
    "    %time label_vec = cPickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sim_cosine_train = np.zeros((train_y_pred.shape[0],label_vec.shape[0]))\n",
    "for i in xrange(train_y_pred.shape[0]):\n",
    "    for j in xrange(label_vec.shape[0]):\n",
    "        sim_cosine_train[i,j] = np.dot(train_y_pred[i],label_vec[j])/np.sqrt((train_y_pred[i]**2).sum())/np.sqrt((label_vec[j]**2).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sim_cosine = np.zeros((y_pred.shape[0],label_vec.shape[0]))\n",
    "for i in xrange(y_pred.shape[0]):\n",
    "    for j in xrange(label_vec.shape[0]):\n",
    "        sim_cosine[i,j] = np.dot(y_pred[i],label_vec[j])/np.sqrt((y_pred[i]**2).sum())/np.sqrt((label_vec[j]**2).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_pred = np.argsort(sim_cosine)[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 100)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_vec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({3: 4550, 7: 574, 2: 362, 6: 215, 4: 201, 9: 42, 8: 29, 0: 21, 1: 6})\n"
     ]
    }
   ],
   "source": [
    "## number of predict classes\n",
    "from collections import Counter\n",
    "print Counter(np.argsort(sim_cosine)[:,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x7f2595032ed0>"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": [
       "iVBORw0KGgoAAAANSUhEUgAAAXsAAAEKCAYAAADzQPVvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\n",
       "AAALEgAACxIB0t1+/AAAG5hJREFUeJzt3X+UXWV97/H3J4RAgJTww4b8UlLXUEnFBrGElqKD0qzg\n",
       "1ZDeawlRELzx14qC16u3Ja57zaj3xh9XVG4tWW3lR4ISjVjSoIESkUFYCikxkcCQQmgimYEMCDGA\n",
       "1DYx3/vHfo7uDGfmnJk5c04mz+e11lnZ+9n72fu7z5n5nD3P3udEEYGZmR3axrS6ADMzG3kOezOz\n",
       "DDjszcwy4LA3M8uAw97MLAMOezOzDDjsMyWpU9KiBmxnvKRbJf1C0rfqWL9d0s7h7rfKdk+WtF9S\n",
       "zZ/p4dRQq6+ksyU9JukFSfOGso8a+79M0j0jsN1zJG1t9Hbt4DG21QVYy0R6DNc7gN8Fjo+I/Q3Y\n",
       "3mj3aeD/RcRft7qQwYiIe4DXtLoOGzk+sx+FJB1Mb9KvAh510P/GK4GuoXSUdFiDazH7DYf9KCFp\n",
       "h6S/lPQg8IKkwyRdKWmbpOclPSxpfmn9yyTdK+n/SnpO0r9KmtvPtidLelDSx/pZfmoa9tkt6SFJ\n",
       "b0/tnwL+F7AgDVu8p0rf8ZJuSDU8DPxRPdtOy05IQ0R7JG2Q9L/rHcKQ9B5JXem5eVzS+6uss0TS\n",
       "M5K2S3pnqf0ISV+U9DNJuyQtl3RkHft8HPg94Na038MlTZG0VtKzaXjnvaX1OyTdLOlGSXuAS6ts\n",
       "84TUf4+k+4FX91n+J5L+OQ2jbZD0x6VlMyT9MNWyXtLfSLqxn9oPGJ5KP28fTz8XL0i6VtIkSbel\n",
       "WtZLmlha/9uSnkp13C1pZp9j6Pd1lPSatL1nJW2V9BelZW9NP9vPS+ru72fU6hARfoyCB7AD+Akw\n",
       "FTgitb0DOClNXwi8CExK85cB/wEsAgR8EOgpbe8u4L8CM4B/Ad7bz34PB7YBV1IM+50LPA+ckpYv\n",
       "BVYOUPfngLuBicA04CHgiTq3/U3gJuBI4FTgCeCH/eznZGA/MCbNvxWYkabfCPwSOD3NtwN7gS+m\n",
       "Gt6YnrvKfr8MrEk1HwOsBZaV+u4c4Hi3A28uzf8Q+CowDvhD4Gng3LSsI71G89L8kVW29830GA/8\n",
       "AdBdeQ6A44HdwLsoTtwuAp4DjkvLfwx8IT23ZwN7+nut+h5XOo4fAa8ApgC9FD9/fwgcAdwJfLK0\n",
       "/mXA0en5/DKwqc8xVH0dU5+dFG90Y4BZwDPAa9Lyp4Cz0/SxldfQjyFkSKsL8KPOF6r45busxjqb\n",
       "SsFxGfBYadlRKQx/N83fBVyVtrtggG2eAzzVp+0mYGma7gBuHKD/48Cc0vz7KqEy0LaBw1IQtpWW\n",
       "fQa4p5/9nEwp7KssvwW4Ik23U4T9+NLybwH/k+KN8UXg90rL/hj411LfusIemA7sA44uLV8GXF96\n",
       "7joH2FblOTil1PZ/Ks8BcAlwX58+P0rB+cp0jEeWlt3Y32vV97jScSwszd8M/E1p/sPALf1sa2J6\n",
       "LSbUeh2BBfR5Awf+lvRGAvwMeD/wO83+nTvUHh7GGV0OuAtE0rslbUpDILuB1wInlFbZVZmIiJfS\n",
       "5DGV7hRnhN3AdwbY55S++6X4BZxaZ819+z9Rx7anACdSnJGWl3fXuU8knS/pvjQ0sJviTL/83OyO\n",
       "iH/rs9/Jab9HARtLz+ttqX2wpgDPRcQvS21PcOBzN9AxvYKXPwd9n78nOFDltZmc9v2r0rLB3oHU\n",
       "W5r+tz7zvyL9LKUhxc+lIcU9FG8UQfGcVTuG8jG/Cphdea7T8/1OYFJa/l8oXrsdabjvrEEegyUO\n",
       "+9HlN3fPSHoV8HfAhyjuhDmOYohEg9jWUuBZ4Cb1f8vik8B0SeXtvor6g/cpirPMivJ0f9vuofhT\n",
       "fh/F2XFFebpfko6geAP7AsVfMscB6zjwuTlO0lF99vsk8HOKYJsZEcelx8SI+J169t3Hk8Dxko4p\n",
       "tb2SA5+7ge6IqjwH/T1/Panusspr81Ta9/h++g5Ffz9b7wTmAW+JiGMphgaVHrVexyeAu0vP9XER\n",
       "MSEiPgQQEQ9ExHyKN401wOphHkO2HPaj19EUQfFzYEy6OPraQW5jL/AXaVsr+4RuxX3AS8BfpguO\n",
       "7cDbKMZh67EaWCJpoqRpwOWlZff3t+0o7u75B6BDxUXe11AMW9Rzu+i49Pg5sF/S+cCcKut9Ku33\n",
       "HOA/Ad+OYuzg74GvSHoFgKSpkqr1H1BE7KQYVvlsuuj7OorrJF+vs/+vOfA5mEkxRFN5Dm4DTpG0\n",
       "UNJYSQsobp/8bkQ8ATyQ+h6eLty+jcbcbtvXMcC/A89JOppiqKq/Y+j7On4vHcPFqc7DJf1Rumh7\n",
       "uKR3STo2becF4NcjUH8WHPajVER0UYy5/5hiuOa1wL3lVXj5L/bLftEjYi/wnyn+bL62b+Cn5W8H\n",
       "zqc4S/sqcElEPDrAfso+RTG0sB24HVhZWT8i/qPGtj9McVFuF7ACWEUx/tufynZfAK6geKN5DlgI\n",
       "/GOfdZ+iuLj5JMVY9gdK+/0rigvH96VhifXAKX33U6eFFNcTnqQIvU9GxA9K26m1rQ9ThOku4Lr0\n",
       "KDpHPEsR4B+jeGP7OPC2iHgurfIuiusNz1KMk3+LOp6/OpeXa19J8Rr3UPx1+eM+6/b7OqbXag7F\n",
       "xeUeitflsxRv1gAXA9vT6/D+dEw2BEoXQaovLG43u5vi6vs44B8jYomkDuC9FL+gAJ+IiNtSnyUU\n",
       "Zy+/prggdkdqPwO4geKK/LqI+MhIHJAduiR9nmJY5mW3eFptKj7h3BURn2pxHX4dW2DAM/t0cefc\n",
       "iJgFvA44V9KfUrxrfykiTk+PStDPpLi6PhOYC1xTOlNcDiyKiDagTf3c821WIen3Jb1OhTMpTiJu\n",
       "aXVdo4WkN0h6taQxaShrHsW4d7Pr8Ot4EKj5SczSXRzjKG6j2p3mq43vXgCsSn/675C0jeJK+8+A\n",
       "CRGxIa23EphP8We9WX8mUPzJX7nP+4sRsba1JY0qJ1EMHZ1AcTfMByPipy2ow6/jQaBm2Ke7NH5C\n",
       "8cm95RHxsKR3AJdLejfFRaCPRcQvKF7M+0rduyluA9vLgXcg9FD/rXuWqYh4AGhrdR2jVUR8F/ju\n",
       "QVCHX8eDQM0LtBGxPw3jTAPemO6YWE5xe9UsigsqV41kkWZmNjx1f6FWROyR9D3gDRHRWWmX9DXg\n",
       "1jTbw4H30E6jOKPvSdPl9p6++5A0EreFmZkd8iJi4M/YDPTxWopPwE1M0+MpvufjLaTvY0ntHwVu\n",
       "StMzgc0U4/szKD4qX7nj535gNsVY/zpgbpX9Ras/UjycB9DR6hpcf+vryLH+0Vz7IVJ/1Fqn1pn9\n",
       "ZGBFGrcfQ/G9GndKWilpFsVdOduBD6S9dUlaTfEVr/uAxZEqARZT3Ho5nuLWS1+cNTNrkgHDPiK2\n",
       "AK+v0v7uAfoso/QJulL7RuC0IdRoZmbD5E/QNlZnqwsYps5WFzBMna0uYJg6W13AMHS2uoBh6mx1\n",
       "ASNtwE/QNpukiFoXGczM7AD1ZKfP7M3MMuCwNzPLgMPezCwDDnszsww47M3MMuCwNzPLgMPezCwD\n",
       "Dnszsww47M3MMuCwNzPLgMPezCwDDnszsww47M3MMuCwNzPLgMPezCwDDnszsww47M3MMuCwNzPL\n",
       "wID/4bjVT9JHYezngSb+t4oR8Os3R8S9zdunmY1GDvvGGQsfFHyhic/pWXvgQf91ZmY1Oewb6vCA\n",
       "8U3cn3PezOozYFpIOlLS/ZI2S+qS9NnUfryk9ZIelXSHpImlPkskPSZpq6Q5pfYzJG1Jy64euUMy\n",
       "M7O+Bgz7iPgVcG5EzAJeB5wr6U+BK4H1EXEKcGeaR9JMYAEwE5gLXCOpMoa9HFgUEW1Am6S5I3FA\n",
       "Zmb2cjXHASLipTQ5DjgM2A3MA1ak9hXA/DR9AbAqIvZGxA5gGzBb0mRgQkRsSOutLPUxM7MRVjPs\n",
       "JY2RtBnoBe6KiIeBSRHRm1bpBSal6SlAd6l7NzC1SntPajczsyaoeYE2IvYDsyQdC/yTpHP7LA9J\n",
       "0aiCJHWUZjsjorNR2zYzOxRIagfaB9On7rtxImKPpO8BZwC9kk6KiF1piObptFoPML3UbRrFGX1P\n",
       "mi639/Szn476yzczy086Ce6szEtaWqtPrbtxTqzcaSNpPPBnwCZgLXBpWu1SYE2aXgtcJGmcpBlA\n",
       "G7AhInYBz0uanS7YXlLqY2ZmI6zWmf1kYIWkMRRvDDdGxJ2SNgGrJS0CdgAXAkREl6TVQBewD1gc\n",
       "EZUhnsXADRQ3oq+LiNsbfTBmZlbdgGEfEVuA11dpfw44r58+y4BlVdo3AqcNrUwzMxsOfwTTzCwD\n",
       "Dnszsww47M3MMuCwNzPLgMPezCwDDnszsww47M3MMuCwNzPLgMPezCwDDnszsww47M3MMuCwNzPL\n",
       "gMPezCwDDnszsww47M3MMuCwNzPLgMPezCwDDnszsww47M3MMuCwNzPLgMPezCwDDnszsww47M3M\n",
       "MjBg2EuaLukuSQ9LekjSFam9Q1K3pE3pcX6pzxJJj0naKmlOqf0MSVvSsqtH7pDMzKyvsTWW7wU+\n",
       "GhGbJR0DbJS0HgjgSxHxpfLKkmYCC4CZwFTg+5LaIiKA5cCiiNggaZ2kuRFxe8OPyMzMXmbAM/uI\n",
       "2BURm9P0i8AjFCEOoCpdLgBWRcTeiNgBbANmS5oMTIiIDWm9lcD8BtRvZmZ1qHvMXtLJwOnAfanp\n",
       "ckk/lXStpImpbQrQXerWTfHm0Le9h9++aZiZ2QirNYwDQBrCuRn4SES8KGk58Om0+DPAVcCiRhQk\n",
       "qaM02xkRnY3YrpnZoUJSO9A+mD41w17S4cB3gK9HxBqAiHi6tPxrwK1ptgeYXuo+jeKMvidNl9t7\n",
       "qu0vIjrqL9/MLD/pJLizMi9paa0+te7GEXAt0BURXym1Ty6t9ufAljS9FrhI0jhJM4A2YENE7AKe\n",
       "lzQ7bfMSYE09B2VmZsNX68z+bOBi4EFJm1LbJ4CFkmZR3JWzHfgAQER0SVoNdAH7gMXpThyAxcAN\n",
       "wHhgne/EMTNrngHDPiLupfrZ/20D9FkGLKvSvhE4bbAFmpnZ8PkTtGZmGXDYm5llwGFvZpYBh72Z\n",
       "WQYc9mZmGXDYm5llwGFvZpYBh72ZWQYc9mZmGXDYm5llwGFvZpYBh72ZWQYc9mZmGXDYm5llwGFv\n",
       "ZpYBh72ZWQYc9mZmGXDYm5llwGFvZpYBh72ZWQYc9mZmGXDYm5llwGFvZpaBAcNe0nRJd0l6WNJD\n",
       "kq5I7cdLWi/pUUl3SJpY6rNE0mOStkqaU2o/Q9KWtOzqkTskMzPrq9aZ/V7goxHxB8BZwIcknQpc\n",
       "CayPiFOAO9M8kmYCC4CZwFzgGklK21oOLIqINqBN0tyGH42ZmVU1YNhHxK6I2JymXwQeAaYC84AV\n",
       "abUVwPw0fQGwKiL2RsQOYBswW9JkYEJEbEjrrSz1MTOzEVb3mL2kk4HTgfuBSRHRmxb1ApPS9BSg\n",
       "u9Stm+LNoW97T2o3M7MmGFvPSpKOAb4DfCQiXvjtyAxEREiKRhUkqaM02xkRnY3atpnZoUBSO9A+\n",
       "mD41w17S4RRBf2NErEnNvZJOiohdaYjm6dTeA0wvdZ9GcUbfk6bL7T3V9hcRHYM5ADOz3KST4M7K\n",
       "vKSltfrUuhtHwLVAV0R8pbRoLXBpmr4UWFNqv0jSOEkzgDZgQ0TsAp6XNDtt85JSHzMzG2G1zuzP\n",
       "Bi4GHpS0KbUtAT4HrJa0CNgBXAgQEV2SVgNdwD5gcURUhngWAzcA44F1EXF7A4/DzMwGMGDYR8S9\n",
       "9H/2f14/fZYBy6q0bwROG2yBZmY2fP4ErZlZBhz2ZmYZcNibmWWgrvvs7aB2d/lzD80QEc3doZkN\n",
       "m8P+kNCwz7TVwTlvNhp5GMfMLAMOezOzDDjszcwy4LA3M8uAw97MLAMOezOzDDjszcwy4LA3M8uA\n",
       "w97MLAMOezOzDDjszcwy4LA3M8uAw97MLAMOezOzDDjszcwy4LA3M8uAw97MLAMOezOzDNQMe0nX\n",
       "SeqVtKXU1iGpW9Km9Di/tGyJpMckbZU0p9R+hqQtadnVjT8UMzPrTz1n9tcDc/u0BfCliDg9PW4D\n",
       "kDQTWADMTH2u0W//N+zlwKKIaAPaJPXdppmZjZCaYR8R9wC7qyyq9j9PXwCsioi9EbED2AbMljQZ\n",
       "mBARG9J6K4H5QyvZzMwGazhj9pdL+qmkayVNTG1TgO7SOt3A1CrtPandzMyaYOwQ+y0HPp2mPwNc\n",
       "BSxqREGSOkqznRHR2YjtmpkdKiS1A+2D6TOksI+Ip0s7/Rpwa5rtAaaXVp1GcUbfk6bL7T39bLtj\n",
       "KDWZmeUinQR3VuYlLa3VZ0jDOGkMvuLPgcqdOmuBiySNkzQDaAM2RMQu4HlJs9MF20uANUPZt5mZ\n",
       "DV7NM3tJq4A3ASdK2gksBdolzaK4K2c78AGAiOiStBroAvYBiyMi0qYWAzcA44F1EXF7g4/FzMz6\n",
       "UTPsI2JhlebrBlh/GbCsSvtG4LRBVWdmZg3hT9CamWXAYW9mlgGHvZlZBhz2ZmYZcNibmWXAYW9m\n",
       "lgGHvZlZBhz2ZmYZcNibmWXAYW9mlgGHvZlZBhz2ZmYZcNibmWXAYW9mlgGHvZlZBhz2ZmYZcNib\n",
       "mWXAYW9mlgGHvZlZBhz2ZmYZcNibmWXAYW9mlgGHvZlZBmqGvaTrJPVK2lJqO17SekmPSrpD0sTS\n",
       "siWSHpO0VdKcUvsZkrakZVc3/lDMzKw/9ZzZXw/M7dN2JbA+Ik4B7kzzSJoJLABmpj7XSFLqsxxY\n",
       "FBFtQJukvts0M7MRUjPsI+IeYHef5nnAijS9Apifpi8AVkXE3ojYAWwDZkuaDEyIiA1pvZWlPmZm\n",
       "NsKGOmY/KSJ603QvMClNTwG6S+t1A1OrtPekdjMza4Kxw91ARISkaEQxAJI6SrOdEdHZqG2bmR0K\n",
       "JLUD7YPpM9Sw75V0UkTsSkM0T6f2HmB6ab1pFGf0PWm63N5TbcMR0THEmszMspBOgjsr85KW1uoz\n",
       "1GGctcClafpSYE2p/SJJ4yTNANqADRGxC3he0ux0wfaSUh8zMxthNc/sJa0C3gScKGkn8Engc8Bq\n",
       "SYuAHcCFABHRJWk10AXsAxZHRGWIZzFwAzAeWBcRtzf2UMzMrD81wz4iFvaz6Lx+1l8GLKvSvhE4\n",
       "bVDVmZlZQ/gTtGZmGXDYm5llwGFvZpYBh72ZWQYc9mZmGXDYm5llwGFvZpYBh72ZWQYc9mZmGRj2\n",
       "t16ajbRGfqvqYESEaq9lNjo47G2UaHbeO+ft0OJhHDOzDDjszcwy4LA3M8uAw97MLAMOezOzDDjs\n",
       "zcwy4LA3M8uAw97MLAMOezOzDDjszcwy4LA3M8uAw97MLAPDCntJOyQ9KGmTpA2p7XhJ6yU9KukO\n",
       "SRNL6y+R9JikrZLmDLd4MzOrz3DP7ANoj4jTI+LM1HYlsD4iTgHuTPNImgksAGYCc4FrJPkvCzOz\n",
       "JmhE2Pb9Lth5wIo0vQKYn6YvAFZFxN6I2AFsA87EzMxGXCPO7L8v6QFJ70ttkyKiN033ApPS9BSg\n",
       "u9S3G5g6zP2bmVkdhvufl5wdEU9JegWwXtLW8sKIiBr/y9DLlknqKM12RkTnMGs0MzukSGoH2gfT\n",
       "Z1hhHxFPpX+fkXQLxbBMr6STImKXpMnA02n1HmB6qfu01NZ3mx3DqcnM7FCXToI7K/OSltbqM+Rh\n",
       "HElHSZqQpo8G5gBbgLXApWm1S4E1aXotcJGkcZJmAG3AhqHu38zM6jecM/tJwC2SKtv5RkTcIekB\n",
       "YLWkRcAO4EKAiOiStBroAvYBiyOiJf+RtJlZboYc9hGxHZhVpf054Lx++iwDlg11n2ZmNjS+z93M\n",
       "LAMOezOzDDjszcwy4LA3M8uAw97MLAMOezOzDDjszcwy4LA3M8uAw97MLAMOezOzDDjszcwy4LA3\n",
       "M8uAw97MLAMOezOzDAz3vyU0swap8V94joiIULP3aa3hsDc7qDQz753zOfEwjplZBhz2ZmYZcNib\n",
       "mWXAYW9mlgGHvZlZBhz2ZmYZaGrYS5oraaukxyT9VTP3bWaWs6aFvaTDgK8Cc4GZwEJJpzZr/82x\n",
       "c5TfuNzZ6gKGqbPVBWRLUnuraxiO0V5/PZp5Zn8msC0idkTEXuCbwAVN3H8T7Bzlw2KdrS5gmDpb\n",
       "XUDO2ltdwDC1t7qAkdbMT9BOBXaW5ruB2U3cv5n10civaJC0tJ71/BUNrdHMsG/69340X9d+ePML\n",
       "zdvfo0c2b192aGrUr2VHetTS/Jyv9w2t3jerehyMb2iKaE4GSzoL6IiIuWl+CbA/Ij5fWieDNwQz\n",
       "s8ar9QbTzLAfC/wL8BbgSWADsDAiHmlKAWZmGWvaME5E7JP0YeCfgMOAax30ZmbN0bQzezMza52D\n",
       "4lZBSddJ6pW0pdW1DIWk6ZLukvSwpIckXdHqmuol6UhJ90vaLKlL0mdbXdNQSDpM0iZJt7a6lsGS\n",
       "tEPSg6n+Da2uZ7AkTZR0s6RH0s/QWa2uqV6Sfj8975XHntH0+wvF9c+UPVsk3STpiKrrHQxn9pLO\n",
       "AV4EVkbEaa2uZ7AknQScFBGbJR0DbATmj5ZhKklHRcRL6brKvcDHI+LeVtc1GJL+O3AGMCEi5rW6\n",
       "nsGQtB04IyKea3UtQyFpBXB3RFyXfoaOjog9ra5rsCSNAXqAMyNiZ631DwaSTgZ+AJwaEf8u6VvA\n",
       "uohY0Xfdg+LMPiLuAXa3uo6hiohdEbE5Tb8IPAJMaW1V9YuIl9LkOIrrKaMqdCRNA94KfI3R+98v\n",
       "jcq6JR0LnBMR10FxbW40Bn1yHvD4aAn65HlgL3BUeqM9iuIN62UOirA/lKR32tOB+1tbSf0kjZG0\n",
       "GegF7oqIrlbXNEhfBv4HsL/VhQxRAN+X9ICk97W6mEGaATwj6XpJP5H095KOanVRQ3QRcFOrixiM\n",
       "9NfgVcATFHc5/iIivl9tXYd9A6UhnJuBj6Qz/FEhIvZHxCxgGvDG0fQ9IZLeBjwdEZsYpWfHwNkR\n",
       "cTpwPvChNKw5WowFXg9cExGvB34JXNnakgZP0jjg7cC3W13LYEh6NfDfgJMpRhOOkfSuaus67BtE\n",
       "0uHAd4CvR8SaVtczFOnP7+8Bb2h1LYPwJ8C8NO69CnizpJUtrmlQIuKp9O8zwC0U3yM1WnQD3RHx\n",
       "z2n+ZorwH23OBzam12A0eQPwo4h4NiL2Af9A8TvxMg77BpAk4FqgKyK+0up6BkPSiZImpunxwJ8B\n",
       "m1pbVf0i4hMRMT0iZlD8Gf6DiHh3q+uql6SjJE1I00cDc4BRc1daROwCdko6JTWdBzzcwpKGaiHF\n",
       "ycJosxU4S9L4lEPnAVWHYZv53Tj9krQKeBNwgqSdwCcj4voWlzUYZwMXAw9KqgTlkoi4vYU11Wsy\n",
       "sCLdiTAGuDEi7mxxTcPR+tvLBmcScEvxe8pY4BsRcUdrSxq0y4FvpKGQx4H3tLieQUlvsucBo+16\n",
       "CRHx0/SX7AMU16x+AvxdtXUPilsvzcxsZHkYx8wsAw57M7MMOOzNzDLgsDczy4DD3swsAw57M7MM\n",
       "OOzNzDLgsDczy8D/B2/eJigAIA7jAAAAAElFTkSuQmCC\n"
      ],
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f2594ea2610>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pylab as pl\n",
    "%matplotlib inline\n",
    "rank = 10 - np.where(np.argsort(sim_cosine) == 5)[1]\n",
    "pl.hist(rank,bins=10)\n",
    "pl.title('rank of dog label for dog images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x7f255eb72ed0>"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": [
       "iVBORw0KGgoAAAANSUhEUgAAAYAAAAEKCAYAAAAb7IIBAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\n",
       "AAALEgAACxIB0t1+/AAAHNtJREFUeJzt3XuYXVWZ5/HvjwByCRIidMgNghqU0LYgI2kRx6g0E9QG\n",
       "mpnmphiGPIqmBYbR0QS6JQ0trd0DA7YPcUa5BJBgWi6iREy4VEujkMEOEAg0iRAhRVLIPY4Kibzz\n",
       "x15Fdk5O1Tl1qursqlq/z/PUk73Xvr1rn13r3XutfSqKCMzMLD/bVR2AmZlVwwnAzCxTTgBmZply\n",
       "AjAzy5QTgJlZppwAzMwy5QTQJpKWSDqljcfbR9JGSWpx+42SpqTpqyRd0I9Y2lZ3SX8n6deSnmly\n",
       "/dclvXUQ4lgr6SODHcNgxd8O6Rx9OE2fI+nbA7Tfps6JpClp3T63g/3ZdijZvuoAchERH23z8Z4C\n",
       "duvH9uVtI/20uq836i7pVGB2RHyg1f31RNI+wH8HJkfE8wO9/z7q1zkbCJI6gGsi4vIq4+jFG+cn\n",
       "Ii5sZgNJVwFPR8TfDFZQORnW2cvaqs9PEkoGI5ge7AM8PwQa/6GibQlIkm8mhyEngB5ImizpRknP\n",
       "SnpO0j+l8u0k/XV6fO2StFDSm9OynSRdm9Z/UdJySXulZR2SZqfpUyX9q6R/lPSCpCckzSwde3dJ\n",
       "l0t6RtI6SRf09Kgp6VBJ90t6WdIGSRel8q0eUdPxL5B0T+reuUXSnpK+m7ZdLmnf0n7rPkZL2kPS\n",
       "j9J5eUHSDyVNLC3vSN0w9wC/Ad7aXXdJ7wS+BbwvxfCCpP+QzqNK+zhO0gM91Hd3SVen46+VdG7K\n",
       "M0cAS4EJad9X9LD9/yid19Oa2Xfpc79IRffSE5I+32wXQPqMfp6uiWck/ZOkHWpW+5ikX6b9/0PN\n",
       "+ThN0qp0vm5LTzqNjvlV4APAN9P5+EYqP0zS/5X0UvrM39fLPtZKmivpkXTsKyS9KS2bkc7hlySt\n",
       "By5Pn8NcSWvS78D3JO1R2t8pkn6Vlp1Tc6z5kq4pzR8u6WfpnD0laZakTwMnA19KdfpBE+fhY5JW\n",
       "pGv8KUnn1VlttqTO9Nl8obRtr/UZESLCPzU/wCjgQeAiYGfgTcBhadlpwGpgCrArcANwdVp2OnAL\n",
       "sBPFHfPBwG5p2V3AaWn6VOA1YHZa77NAZ+n4NwEL0rH3Au4DPtNDrD8HPpGmdwGmp+kpwOvAdmm+\n",
       "A3gc2A94M/BIqseHU30XAleU9vs68NY0fSVwQZoeC/xFquNoYDFwU2m7DmAtcADFDcb2NXWfBdxd\n",
       "U4dHgJk19T+7h/penZbvCuwL/Htp3x+k6B7o6XOdCWwApqVzdV1NPXvb92dTnBOAMcDtwB+6z2+d\n",
       "Yz0JfDhNvwc4NJ2PfYFVwFk15/qOtN/J6biz07Jj0uf0jrT9ucA99T6nOjG8cd5Ln92LwCfSvk4E\n",
       "XgDG9rD9WuAhYCKwB/CvpetgBrAJ+Htgh3Q9nAX8LJ2jHSiS/XVp/WnARuBwYEeK361NpXN0HkV3\n",
       "FekcvQKcQHFtjgXeXboWz2/w+1v+TD8IHJim35U+/2Nqfke+S/G79sfAs8BH0vLe6tO9bd3Pf7j8\n",
       "VB7AUPwB3pcuhG0+3PSL+tnS/P4Ujfko4L8C9wDvqrNdbQJYXVq2S7qY/ggYB/we2Km0/CTgzh5i\n",
       "/RdgPrBnTflWF2g6/rzS8v8J3Fqa/ziwojRfNwHUOf5BwAs19ZzfoO61CeDLwLVpeizw/4BxdY41\n",
       "CngVeGep7DPAXWl6Br0ngCuAC0vzU7vr2cS+7wQ+XVr2kd4aAEoJoM6y/wbcWHOujyzNfw64PU3/\n",
       "mK0b8e3S+Zlc+zn1cM3NLs2fAtxbs87PgFm91OEzpfmjgDWlc/0qsGNp+apynYHxbPnd+Aqp8Sxd\n",
       "86+yJQHMZ0sCmAfc0ENMPV6L9a7dOssuAS6u+R3Zv7T868B30vSjPdRnO0ZIAnAXUH2TgV9FxOt1\n",
       "lo0HflWaf4riLvePgGuAnwDXp0fKr6vnvtEN3RMR8ds0OZri7mcHYH16/H2R4s5jrx72M5siCT2a\n",
       "Huk/1ku9ukrTv6dIcuX50b1sC4CkXST979Q98DJFAtq93GUBPN1oPzW+C/y5pF2A44GfRkRXnfX2\n",
       "pDg3ted/Yp116xlfE9tTfdh37bbrmjwmkvZX0W22Pp2zrwJvqVmtNq4JaXpf4NLStdA9vtFsncvj\n",
       "ABPYus5Q1Le3ffUUF8CvI+K10vwU4KZSrKuAzRQ3NeMpnbN0zfc0VjMZeKKXmJomabqku1K33ksU\n",
       "T+l9Ofc91WdEcAKo72lgH0mj6ix7huJC77YPxUXRFRGbI+L8iDgQOIzirvpTLRz7VeAtEbFH+tk9\n",
       "It5Vb+WIWBMRJ0fEXhR3L9+XtHMTx+nrAGH3+l+gSDiHRsTuFI/YYutB4t72vc2yiFgH3AscB3yS\n",
       "IpHW8xxFt8GUUtk+NN8Yr0/rl7dtdt/rKRqmbuXpRhZQNB5vT+fsXLb93auNqzNNP0VxF75H6WfX\n",
       "iLi3iePWnutOikatbF96P3+1cZVfr63d/1MUXXnlWHeJiGeoOX8p2dc2xOX9vK2HZX29bq8DbgYm\n",
       "RcQYipupvpz7evVZ38cYhiwngPruo7hgv5bueHeSdFhatgg4W8Ug62jgQuD6iHg9DYy9KyWOjRQN\n",
       "yh/6cuB0cS0FLpa0Wxp8fJuk/1hvfUmfVBpoBl6m+AWp9+QCWzfSfXk7p9zAjwZ+B7wsaSxF321v\n",
       "x6nVBUyqMwh6NUVX0B8DN9bbMCL+QDHm8FVJo1UMWp8NXNtkPRYDp0o6IDVAb8TexL4XA2dJmiBp\n",
       "TIq12cZoNMX18FsVA+Gfq7POFyWNkTQZOBP4Xir/FnCOpGnwxkD1XzZ53C62bkiXAPtLOknS9pJO\n",
       "AN4J/KiH7QXMkTQxfdbnAtf3crxvARd2D1JL2kvS0WnZ94GPS3q/pB2B8+m5/bkOOELSX6Y43yLp\n",
       "3aU69eV7D6OBFyPiNUmHUgwi135ufy1pZ0kHUnRRls99T/UZEZwA6khdP38OvJ3iLuBpiq4JKPqR\n",
       "rwF+SvGY+lvgjLRsb+CfKRriVRQDovXuZoNtL8Ly/KcoBspWUQzS/XPadz3/CXhY0kbgfwEnRsSr\n",
       "dfZZO98ohp7WvYRiwOw5iv7jHzfYT607KAZTN0gqd0HdSHH3dVNE/L6X7c+g6AN/AribovvoymaO\n",
       "HRG3pfjvpBgQv6Nm/d72/W2KxPwQ8AvgVuAPPXQT1voiRcPzCvB/KBrR2jh/kPa7gqJBviLFfDPF\n",
       "k931qftoJcVn3rC+wKXAf1HxBs8lEfECxVPpFyg+vy8CH0/l9QRFY7wU+CXFYPTf9XLsSyleglgq\n",
       "6RWKFxQOTfVYBfxV2t8zFNd1uevljWssiu+wfDTF+Xw6J3+S1rscmJa6ZereKNTENQc4P8XzN2xp\n",
       "3Mvr/guwhmJg/x8j4vZG9emh/sOO0uBG/YXSThQn500UDdIPImJeuhv4HsXj41rg+Ih4KW0zj+JN\n",
       "mT8AZ0bE0lR+CHAVxdsCSyLirEGqkw1TklYDp0fEnVXH0oiko4AFETGl6lgGi6QnKQaRh/znYa3p\n",
       "9Qkg3Yl9KCIOosjAH5J0ODAXWBYR+1PcRc0FSI+pJ1C88jUTuKw0OLiA4mKaCkxV6b13M0nHATFU\n",
       "G5vUDfjR1CUxkaL7qKc7ULNhoWEXUOkNlR0pXud6ETia4r1x0r/HpuljgEURsSki1lI8Vk2XNJ7i\n",
       "ffjlab2rS9tY5lT8yYLLKLoIhipRvKr4AvBvFN1YX6kyILP+avj1bRXfdPw3isGkBRHxiKRxpdf0\n",
       "utjyWtQEirc5uq2jeMVsE1u/adBJ86+x2QgXETOqjqGRiPgdW/f/jngRsV/VMdjgapgA0iDXQZJ2\n",
       "B34i6UM1y0PSsB8MMTPLTdN/wCkiXpZ0K3AI0CVp74jYkLp3ut/m6GTr96MnUdz5d6bpcnknNZxI\n",
       "zMxaExF9/sOLvSYASXsCmyPipfTloj8D/pbi1ahZFK+nzaL4ogWp/DpJF1N08UwFlqenhFckTQeW\n",
       "U3wl/RsDVYnhQtL8iJhfdRyDxfUbvkZy3SCL+rV089zoCWA8sDCNA2xH8bc67pC0Alis4q9briW9\n",
       "Ix8RqyQtZstXpufElvdM51C8BrozxWugt7USsJmZDYxeE0BErKT4S4a15S8AR/SwzYUU346tLf8F\n",
       "xV/jMzOzIcDfBG6vjqoDGGQdVQcwyDqqDmAQdVQdwCDrqDqAoajXbwK3m6QYyWMAZmaDodW2008A\n",
       "ZmaZcgIwM8uUE4CZWaacAMzMMuUEYGaWKScAM7NMOQGYmWXKCcDMLFNOAGZmmXICMDPLlBOAmVmm\n",
       "nADMzDLlBGBmliknADOzTDkBmJllqun/FN7y1ur/OTrQ/P9FmA0cJwDrg6pzgNt+s4HkLiAzs0w5\n",
       "AZiZZcoJwMwsU04AZmaZcgIwM8uUE4CZWaacAMzMMuUEYGaWKScAM7NM9ZoAJE2WdJekRyQ9LOnM\n",
       "VD5f0jpJK9LPUaVt5klaLekxSUeWyg+RtDItu3TwqmRmZs1QRM9f75e0N7B3RDwgaTTwC+BY4Hhg\n",
       "Y0RcXLP+NOA64L3AROB2YGpEhKTlwOcjYrmkJcA3IuK2mu3Df+tlaCr+FlD1fwrC14fZtlptO3t9\n",
       "AoiIDRHxQJr+DfAoRcMO9f8wyzHAoojYFBFrgTXAdEnjgd0iYnla72qKRGJmZhVpegxA0hTgYODe\n",
       "VHSGpAclXS5pTCqbAKwrbbaOImHUlneyJZGYmVkFmkoAqfvn+8BZ6UlgAbAfcBCwHrho0CI0M7NB\n",
       "0fDPQUvaAbgBuDYibgaIiGdLy78D/DDNdgKTS5tPorjz70zT5fLOHo43vzTbEREdjWI0M8uJpBnA\n",
       "jH7vp8EgsICFwPMRcXapfHxErE/TZwPvjYiTS4PAh7JlEPjtaRD4PuBMYDlwKx4EHlY8CGw2dLXa\n",
       "djZ6Ang/8EngIUkrUtk5wEmSDqJoEZ4ETgeIiFWSFgOrgM3AnNiSYeYAVwE7A0tqG38zM2uvXp8A\n",
       "2s1PAEOXnwDMhq5BeQ3UzMxGLicAM7NMOQGYmWXKCcDMLFNOAGZmmXICMDPLlBOAmVmmnADMzDLl\n",
       "BGBmliknADOzTDkBmJllygnAzCxTTgBmZplyAjAzy5QTgJlZppwAzMwy5QRgZpYpJwAzs0w5AZiZ\n",
       "ZcoJwMwsU04AZmaZcgIwM8uUE4CZWaacAMzMMuUEYGaWKScAM7NMOQGYmWXKCcDMLFNOAGZmmeo1\n",
       "AUiaLOkuSY9IeljSmal8rKRlkh6XtFTSmNI28yStlvSYpCNL5YdIWpmWXTp4VTIzs2Y0egLYBJwd\n",
       "EQcCfwr8laQDgLnAsojYH7gjzSNpGnACMA2YCVwmSWlfC4DZETEVmCpp5oDXxszMmtZrAoiIDRHx\n",
       "QJr+DfAoMBE4GliYVlsIHJumjwEWRcSmiFgLrAGmSxoP7BYRy9N6V5e2MTOzCjQ9BiBpCnAwcB8w\n",
       "LiK60qIuYFyangCsK222jiJh1JZ3pnIzM6vI9s2sJGk0cANwVkRs3NKrAxERkmKgApI0vzTbEREd\n",
       "A7VvM7ORQNIMYEZ/99MwAUjagaLxvyYibk7FXZL2jogNqXvn2VTeCUwubT6J4s6/M02XyzvrHS8i\n",
       "5vepBmZmmUk3xh3d85LOa2U/jd4CEnA5sCoiLiktugWYlaZnATeXyk+UtKOk/YCpwPKI2AC8Iml6\n",
       "2ucppW3MzKwCiui590bS4cBPgYeA7hXnAcuBxcA+wFrg+Ih4KW1zDnAasJmiy+gnqfwQ4CpgZ2BJ\n",
       "RJxZ53gREaott+oV3XwD1tPXahT4+jDbVqttZ68JoN2cAIYuJwCzoavVttPfBDYzy5QTgJlZppwA\n",
       "zMwy5QRgZpYpJwAzs0w5AZiZZcoJwMwsU04AZmaZcgIwM8uUE4CZWaacAMzMMuUEYGaWKScAM7NM\n",
       "OQGYmWXKCcDMLFNOAGZmmXICMDPLlBOAmVmmnADMzDLlBGBmliknADOzTDkBmJllygnAzCxTTgBm\n",
       "ZplyAjAzy5QTgJlZppwAzMwy5QRgZpaphglA0hWSuiStLJXNl7RO0or0c1Rp2TxJqyU9JunIUvkh\n",
       "klamZZcOfFXMzKwvmnkCuBKYWVMWwMURcXD6+TGApGnACcC0tM1lkpS2WQDMjoipwFRJtfs0M7M2\n",
       "apgAIuJu4MU6i1Sn7BhgUURsioi1wBpguqTxwG4RsTytdzVwbGshm5nZQOjPGMAZkh6UdLmkMals\n",
       "ArCutM46YGKd8s5UbmZmFdm+xe0WAOen6QuAi4DZAxGQpPml2Y6I6BiI/ZqZjRSSZgAz+ruflhJA\n",
       "RDxbCuQ7wA/TbCcwubTqJIo7/840XS7v7GHf81uJycwsF+nGuKN7XtJ5reynpS6g1Kff7S+A7jeE\n",
       "bgFOlLSjpP2AqcDyiNgAvCJpehoUPgW4uZVjm5nZwGj4BCBpEfBBYE9JTwPnATMkHUTxNtCTwOkA\n",
       "EbFK0mJgFbAZmBMRkXY1B7gK2BlYEhG3DXBdzMysD7Slfa6epIiIem8XWcUkRZHvK40CXx9m22q1\n",
       "7fQ3gc3MMuUEYGaWKScAM7NMOQGYmWXKCcDMLFNOAGZmmXICMDPLlBOAmVmmnADMzDLlBGBmlikn\n",
       "ADOzTDkBmJllygnAzCxTTgBmZplyAjAzy5QTgJlZppwAzMwy5QRgZpYpJwAzs0w5AZiZZcoJwMws\n",
       "U04AZmaZcgIwM8uUE4CZWaacAMzMMuUEYGaWKScAM7NMOQGYmWWqYQKQdIWkLkkrS2VjJS2T9Lik\n",
       "pZLGlJbNk7Ra0mOSjiyVHyJpZVp26cBXxczM+qKZJ4ArgZk1ZXOBZRGxP3BHmkfSNOAEYFra5jJJ\n",
       "StssAGZHxFRgqqTafZqZWRs1TAARcTfwYk3x0cDCNL0QODZNHwMsiohNEbEWWANMlzQe2C0ilqf1\n",
       "ri5tY2ZmFWh1DGBcRHSl6S5gXJqeAKwrrbcOmFinvDOVm5lZRbbv7w4iIiTFQAQDIGl+abYjIjoG\n",
       "at9mZiOBpBnAjP7up9UE0CVp74jYkLp3nk3lncDk0nqTKO78O9N0ubyz3o4jYn6LMZmZZSHdGHd0\n",
       "z0s6r5X9tNoFdAswK03PAm4ulZ8oaUdJ+wFTgeURsQF4RdL0NCh8SmkbMzOrQMMnAEmLgA8Ce0p6\n",
       "GvgK8DVgsaTZwFrgeICIWCVpMbAK2AzMiYju7qE5wFXAzsCSiLhtYKtiZmZ9oS3tc/UkRUSo8ZrW\n",
       "bsU4T9XXivD1YbatVttOfxPYzCxTTgBmZplyAjAzy5QTgJlZppwAzMwy5QRgZpYpJwAzs0w5AZiZ\n",
       "ZcoJwMwsU04AZmaZcgIwM8uUE4CZWaacAMzMMuUEYGaWKScAM7NMOQGYmWXKCcDMLFNOAGZmmWr4\n",
       "fwKbDSXFf01ZLf+3lDZSOAHYMFN1+++230YOdwGZmWXKCcDMLFNOAGZmmXICMDPLlBOAmVmmnADM\n",
       "zDLl10DN+sjfRbCRwgnArM+qbv/d9tvA6FcXkKS1kh6StELS8lQ2VtIySY9LWippTGn9eZJWS3pM\n",
       "0pH9Dd7MzFrX3zGAAGZExMERcWgqmwssi4j9gTvSPJKmAScA04CZwGWSPAZhZlaRgWiAa59HjwYW\n",
       "pumFwLFp+hhgUURsioi1wBrgUMzMrBID8QRwu6T7JX06lY2LiK403QWMS9MTgHWlbdcBE/t5fDMz\n",
       "a1F/B4HfHxHrJe0FLJP0WHlhRESDNyaqHk0b8obCGydmNjL1KwFExPr0768l3UTRpdMlae+I2CBp\n",
       "PPBsWr0TmFzafFIq24qk+aXZjojo6E+MI8NQyAF+88RsqJA0A5jR7/1EtNa4SNoFGBURGyXtCiwF\n",
       "/hY4Ang+Ir4uaS4wJiLmpkHg6yiSxETgduDtUQpAUvj95q0VTwBDJQFUHYdj6I7BvydW1mrb2Z8n\n",
       "gHHATZK69/PdiFgq6X5gsaTZwFrgeICIWCVpMbAK2AzMiVazj5mZ9VvLTwCDwU8A2/ITgGOoF4N/\n",
       "T6ys1bbT7+GbmWXKCcDMLFP+W0Bm1mdD5fVkd4X1jxOAmbWo6hzgtr+/3AVkZpYpJwAzs0y5C8hs\n",
       "GBoqffBVGwrnYTiPQzgBmA1LVbd7Q6XN83noD3cBmZllygnAzCxTTgBmZplyAjAzy5QHgc3M+mEo\n",
       "vInUKicAM7N+qbr9b/1NJHcBmZllygnAzCxTTgBmZplyAjAzy5QTgJlZppwAzMwy5QRgZpYpJwAz\n",
       "s0w5AZiZZcoJwMwsU04AZmaZcgIwM8uUE4CZWaacAMzMMtXWBCBppqTHJK2W9OV2HrsVkqLqn6rP\n",
       "gZmNXG1LAJJGAd8EZgLTgJMkHdCu47cuBvDnrha2GU46qg5gkHVUHcAg6qg6gEHWUXUAQ1I7nwAO\n",
       "BdZExNqI2ARcDxzTxuMPAR1VBzDIOqoOYJB1VB3AIOqoOoBB1lF1AENSO/9HsInA06X5dcD0eitK\n",
       "Og5GXQ9q/b+6MTOzXrUzAfSlP2M72PV1OPC1QYumoY2j4GH/l5lmNmIpoj39zJL+FJgfETPT/Dzg\n",
       "9Yj4emmd4dbpbWY2JEREn3tM2pkAtgf+HfgI8AywHDgpIh5tSwBmZraVtnVxRMRmSZ8HfgKMAi53\n",
       "429mVp22PQGYmdnQUuk3gSWNlbRM0uOSlkoaU2edyZLukvSIpIclnVlFrH3RzBfeJH0jLX9Q0sHt\n",
       "jrE/GtVP0idSvR6SdI+kP6kizlY0+2VFSe+VtLl4Y234aPLanCFpRfp962hziP3SxLW5p6TbJD2Q\n",
       "6ndqBWG2RNIVkrokrexlnb61KxFR2Q/wD8CX0vSXga/VWWdv4KA0PZpiHOGAKuNuUKdRwBpgCrAD\n",
       "8EBtvMBHgSVpejpwb9VxD3D93gfsnqZnDpf6NVO30np3Aj8C/nPVcQ/wZzcGeASYlOb3rDruAa7f\n",
       "fODvu+sGPA9sX3XsTdbvA8DBwMoelve5Xan6bwEdDSxM0wuBY2tXiIgNEfFAmv4N8CgwoW0R9l0z\n",
       "X3h7o94RcR8wRtK49obZsob1i4ifR8TLafY+YFKbY2xVs19WPAP4PvDrdgY3AJqp38nADRGxDiAi\n",
       "nmtzjP3RTP3WA29O028Gno+IzW2MsWURcTfwYi+r9LldqToBjIuIrjTdBfQarKQpFBnwvsENq1/q\n",
       "feFtYhPrDJdGspn6lc0GlgxqRAOnYd0kTaRoVBakouE0iNbMZzcVGJu6Xe+XdErbouu/Zur3beBA\n",
       "Sc8ADwJntSm2duhzuzLobwFJWkbRjVPr3PJMRPT6x88kjaa46zorPQkMVc02CLXv7A6XhqTpOCV9\n",
       "CDgNeP/ghTOgmqnbJcDcdL2KbT/HoayZ+u0AvIfide1dgJ9LujciVg9qZAOjmfqdAzwQETMkvQ1Y\n",
       "JundEbFxkGNrlz61K4OeACLiz3palgY09o6IDZLGA8/2sN4OwA3AtRFx8yCFOlA6gcml+ckUmbi3\n",
       "dSalsuGgmfqRBn6/DcyMiN4eW4eSZup2CHB90fazJ3CUpE0RcUt7QuyXZur3NPBcRPwO+J2knwLv\n",
       "BoZDAmimfocBXwWIiF9KehJ4B3B/WyIcXH1uV6ruAroFmJWmZwHbNO7pLutyYFVEXNLG2Fp1PzBV\n",
       "0hRJOwInUNSz7BbgU/DGN6RfKnWFDXUN6ydpH+BG4JMRsaaCGFvVsG4R8daI2C8i9qN4Iv3cMGn8\n",
       "oblr8wfA4ZJGSdqFYjBxVZvjbFUz9XsMOAIg9Y+/A3iirVEOnr63KxWPao8FbgceB5YCY1L5BODW\n",
       "NH048DrFiP6K9DOz6hH5BvU6iuJtpTXAvFR2OnB6aZ1vpuUPAu+pOuaBrB/wHYq3K7o/r+VVxzyQ\n",
       "n11p3SuB46qOeaDrB3yR4k2glcCZVcc8kPWjeGr7Yfq9WwmcXHXMfajbIoq/ovAaxZPaaf1tV/xF\n",
       "MDOzTFXdBWRmZhVxAjAzy5QTgJlZppwAzMwy5QRgZpYpJwAzs0w5AZiZZcoJwMwsU/8ffxKjNao/\n",
       "iC0AAAAASUVORK5CYII=\n"
      ],
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f25933db910>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pylab as pl\n",
    "%matplotlib inline\n",
    "pl.hist(sim_cosine[np.where(np.argsort(sim_cosine) == 5)])\n",
    "pl.title('cosine similarity of dog label to predict label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cosine_dog = np.zeros((train_x.shape[0] + test_x.shape[0],))\n",
    "tmp = np.concatenate((train_y_pred, y_pred))\n",
    "for i in xrange(cosine_dog.shape[0]):\n",
    "    cosine_dog[i] = np.dot(label_vec[5],tmp[i])/np.sqrt((label_vec[5]**2).sum())/np.sqrt((tmp[i]**2).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"/home/ce-ruoxu/workspace/data-fusion/data/cifar-10-batches-py/batches.meta\",\"r\") as f:\n",
    "    meta = cPickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 airplane\n",
      "1 automobile\n",
      "2 bird\n",
      "3 cat\n",
      "4 deer\n",
      "5 dog\n",
      "6 frog\n",
      "7 horse\n",
      "8 ship\n",
      "9 truck\n"
     ]
    }
   ],
   "source": [
    "j = 0\n",
    "info = []\n",
    "for i in meta[\"label_names\"]:\n",
    "    print j, i\n",
    "    info.append((j,i))\n",
    "    j+= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels = np.zeros((train_y.shape[0],))\n",
    "for i in xrange(train_y.shape[0]):\n",
    "    for j in xrange(label_vec.shape[0]):\n",
    "        if (label_vec[j] == train_y[i]).all():\n",
    "            labels[i] = j\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog\n",
      "horse\n",
      "cat\n",
      "dog\n",
      "dog\n",
      "dog\n",
      "dog\n",
      "dog\n",
      "cat\n",
      "cat\n",
      "truck\n",
      "cat\n",
      "cat\n",
      "cat\n",
      "dog\n",
      "dog\n",
      "dog\n",
      "cat\n",
      "dog\n",
      "dog\n",
      "dog\n",
      "horse\n",
      "dog\n",
      "dog\n",
      "dog\n",
      "cat\n",
      "cat\n",
      "dog\n",
      "cat\n",
      "cat\n",
      "dog\n",
      "dog\n",
      "cat\n",
      "horse\n",
      "cat\n",
      "horse\n",
      "dog\n",
      "dog\n",
      "cat\n",
      "cat\n",
      "dog\n",
      "dog\n",
      "cat\n",
      "cat\n",
      "cat\n",
      "dog\n",
      "dog\n",
      "dog\n",
      "deer\n",
      "cat\n",
      "horse\n",
      "dog\n",
      "dog\n",
      "dog\n",
      "dog\n",
      "cat\n",
      "dog\n",
      "dog\n",
      "cat\n",
      "dog\n",
      "dog\n",
      "dog\n",
      "deer\n",
      "dog\n",
      "dog\n",
      "dog\n",
      "bird\n",
      "cat\n",
      "deer\n",
      "deer\n",
      "cat\n",
      "cat\n",
      "dog\n",
      "dog\n",
      "dog\n",
      "cat\n",
      "horse\n",
      "cat\n",
      "dog\n",
      "horse\n",
      "dog\n",
      "frog\n",
      "dog\n",
      "cat\n",
      "cat\n",
      "dog\n",
      "cat\n",
      "cat\n",
      "cat\n",
      "dog\n",
      "horse\n",
      "dog\n",
      "dog\n",
      "deer\n",
      "dog\n",
      "deer\n",
      "deer\n",
      "deer\n",
      "dog\n",
      "horse\n",
      "cat\n",
      "dog\n",
      "cat\n",
      "cat\n",
      "cat\n",
      "cat\n",
      "dog\n",
      "deer\n",
      "dog\n",
      "dog\n",
      "dog\n",
      "cat\n",
      "cat\n",
      "cat\n",
      "cat\n",
      "dog\n",
      "dog\n",
      "cat\n",
      "dog\n",
      "horse\n",
      "cat\n",
      "dog\n",
      "dog\n",
      "dog\n",
      "deer\n",
      "dog\n",
      "cat\n",
      "deer\n",
      "bird\n",
      "airplane\n",
      "dog\n",
      "deer\n",
      "cat\n",
      "deer\n",
      "dog\n",
      "dog\n",
      "dog\n",
      "horse\n",
      "cat\n",
      "cat\n",
      "cat\n",
      "dog\n",
      "cat\n",
      "dog\n",
      "cat\n",
      "cat\n",
      "dog\n",
      "cat\n",
      "dog\n",
      "dog\n",
      "bird\n",
      "deer\n",
      "horse\n",
      "dog\n",
      "horse\n",
      "horse\n",
      "cat\n",
      "cat\n",
      "dog\n",
      "horse\n",
      "dog\n",
      "dog\n",
      "deer\n",
      "deer\n",
      "horse\n",
      "cat\n",
      "deer\n",
      "cat\n",
      "dog\n",
      "dog\n",
      "cat\n",
      "bird\n",
      "cat\n",
      "cat\n",
      "cat\n",
      "dog\n",
      "deer\n",
      "cat\n",
      "dog\n",
      "cat\n",
      "dog\n",
      "dog\n",
      "cat\n",
      "cat\n",
      "truck\n",
      "deer\n",
      "cat\n",
      "dog\n",
      "horse\n",
      "dog\n",
      "dog\n",
      "cat\n",
      "dog\n",
      "dog\n",
      "cat\n",
      "cat\n",
      "dog\n",
      "cat\n",
      "dog\n",
      "cat\n"
     ]
    }
   ],
   "source": [
    "for i in np.argsort(cosine_dog)[::-1][:200]:\n",
    "    if i > train_x.shape[0] - 1:\n",
    "        print 'dog'\n",
    "    else:\n",
    "        j = int(labels[i])\n",
    "        print info[j][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## using im2tex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"/home/ce-ruoxu/workspace/data-fusion/data/im2text/features\",\"r\") as f:\n",
    "    im2text_x = cPickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"/home/ce-ruoxu/workspace/data-fusion/data/im2text/im2text_y\",\"r\") as f:\n",
    "    im2text_y = cPickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"/home/ce-ruoxu/workspace/data-fusion/data/im2text/sentences.txt\",\"r\") as f:\n",
    "    sentences = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"data/cifar-10-batches-py/test_x\",\"r\") as f:\n",
    "    %time test_x = cPickle.load(f)\n",
    "with open(\"data/cifar-10-batches-py/test_y\",\"r\") as f:\n",
    "    %time test_y = cPickle.load(f)\n",
    "#train_x,train_y = test_x, test_y\n",
    "with open(\"data/cifar-10-batches-py/train_x\",\"r\") as f:\n",
    "    %time train_x = cPickle.load(f)\n",
    "with open(\"data/cifar-10-batches-py/train_y\",\"r\") as f:\n",
    "    %time train_y = cPickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import Doc2Vec\n",
    "model = Doc2Vec.load_word2vec_format(\"/home/ce-ruoxu/workspace/data-fusion/data/im2text/phrase_vectors.bin\",binary = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "train_X,valid_X, train_Y, valid_Y = train_test_split(im2text_x, im2text_y,\n",
    "                                                     test_size = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8310, 4096)\n",
      "(8310, 100)\n",
      "(924, 4096)\n",
      "(924, 100)\n"
     ]
    }
   ],
   "source": [
    "print train_X.shape\n",
    "print train_Y.shape\n",
    "print valid_X.shape\n",
    "print valid_Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... building the model...\n",
      "..training the model...\n",
      "epoch 1, minibatch 166/166, validation error 5235.000784\n",
      "epoch 1,minibatch 166/166, train_error 57.017704\n",
      "epoch 1, minibatch 166/166, test error of best model 796321.124771\n",
      "epoch 2, minibatch 166/166, validation error 5162.757076\n",
      "epoch 2,minibatch 166/166, train_error 51.474007\n",
      "epoch 2, minibatch 166/166, test error of best model 796951.298647\n",
      "epoch 3, minibatch 166/166, validation error 5141.903967\n",
      "epoch 3,minibatch 166/166, train_error 49.750883\n",
      "epoch 3, minibatch 166/166, test error of best model 797496.258644\n",
      "epoch 4, minibatch 166/166, validation error 5142.072509\n",
      "epoch 4,minibatch 166/166, train_error 48.641848\n",
      "epoch 5, minibatch 166/166, validation error 5155.156144\n",
      "epoch 5,minibatch 166/166, train_error 47.763960\n",
      "epoch 6, minibatch 166/166, validation error 5171.989272\n",
      "epoch 6,minibatch 166/166, train_error 46.988105\n",
      "epoch 7, minibatch 166/166, validation error 5197.179803\n",
      "epoch 7,minibatch 166/166, train_error 46.256270\n",
      "epoch 8, minibatch 166/166, validation error 5224.326746\n",
      "epoch 8,minibatch 166/166, train_error 45.554797\n",
      "epoch 9, minibatch 166/166, validation error 5254.724918\n",
      "epoch 9,minibatch 166/166, train_error 44.868970\n",
      "epoch 10, minibatch 166/166, validation error 5289.657578\n",
      "epoch 10,minibatch 166/166, train_error 44.200842\n",
      "epoch 11, minibatch 166/166, validation error 5327.565118\n",
      "epoch 11,minibatch 166/166, train_error 43.556810\n",
      "epoch 12, minibatch 166/166, validation error 5368.647760\n",
      "epoch 12,minibatch 166/166, train_error 42.934359\n",
      "epoch 13, minibatch 166/166, validation error 5406.688219\n",
      "epoch 13,minibatch 166/166, train_error 42.352029\n",
      "epoch 14, minibatch 166/166, validation error 5448.021166\n",
      "epoch 14,minibatch 166/166, train_error 41.806012\n",
      "epoch 15, minibatch 166/166, validation error 5487.636854\n",
      "epoch 15,minibatch 166/166, train_error 41.287356\n",
      "epoch 16, minibatch 166/166, validation error 5528.243431\n",
      "epoch 16,minibatch 166/166, train_error 40.799631\n",
      "epoch 17, minibatch 166/166, validation error 5574.981612\n",
      "epoch 17,minibatch 166/166, train_error 40.348476\n",
      "epoch 18, minibatch 166/166, validation error 5610.897269\n",
      "epoch 18,minibatch 166/166, train_error 39.937739\n",
      "epoch 19, minibatch 166/166, validation error 5645.081913\n",
      "epoch 19,minibatch 166/166, train_error 39.557275\n",
      "epoch 20, minibatch 166/166, validation error 5682.897981\n",
      "epoch 20,minibatch 166/166, train_error 39.199696\n",
      "epoch 21, minibatch 166/166, validation error 5726.378249\n",
      "epoch 21,minibatch 166/166, train_error 38.881559\n",
      "epoch 22, minibatch 166/166, validation error 5778.034346\n",
      "epoch 22,minibatch 166/166, train_error 38.666822\n",
      "epoch 23, minibatch 166/166, validation error 5787.704608\n",
      "epoch 23,minibatch 166/166, train_error 38.603544\n",
      "epoch 24, minibatch 166/166, validation error 5822.774336\n",
      "epoch 24,minibatch 166/166, train_error 38.640569\n",
      "epoch 25, minibatch 166/166, validation error 5863.479485\n",
      "epoch 25,minibatch 166/166, train_error 38.605192\n",
      "epoch 26, minibatch 166/166, validation error 5846.758180\n",
      "epoch 26,minibatch 166/166, train_error 38.430204\n",
      "epoch 27, minibatch 166/166, validation error 5833.800916\n",
      "epoch 27,minibatch 166/166, train_error 38.052476\n",
      "epoch 28, minibatch 166/166, validation error 5884.261873\n",
      "epoch 28,minibatch 166/166, train_error 37.749671\n",
      "epoch 29, minibatch 166/166, validation error 5908.106844\n",
      "epoch 29,minibatch 166/166, train_error 37.559900\n",
      "epoch 30, minibatch 166/166, validation error 5870.000152\n",
      "epoch 30,minibatch 166/166, train_error 37.388394\n",
      "epoch 31, minibatch 166/166, validation error 5925.428274\n",
      "epoch 31,minibatch 166/166, train_error 37.115770\n",
      "CPU times: user 20min 10s, sys: 18min 55s, total: 39min 5s\n",
      "Wall time: 2min 25s\n"
     ]
    }
   ],
   "source": [
    "mapping = MAPPING(n_hidden = 200, n_epochs = 100, batch_size = 50, learning_rate=0.01, lam=1e-4, beta = 5)\n",
    "%time mapping.fit(train_X,train_Y, valid_X, valid_Y, np.concatenate((train_x, test_x)), np.concatenate((train_y,test_y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
